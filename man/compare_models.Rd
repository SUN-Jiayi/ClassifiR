% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation.R
\name{compare_models}
\alias{compare_models}
\title{Compare Multiple Binary Classification Models and Identify Best by Each Criterion}
\usage{
compare_models(y_true, pred_list, prob_list = NULL)
}
\arguments{
\item{y_true}{A numeric vector of true binary labels (0/1).}

\item{pred_list}{A named list of numeric vectors.
Each element contains predicted labels (0/1) for a model.
The name of each element will be used to identify the model.}

\item{prob_list}{Optional named list of numeric vectors with predicted probabilities (values in [0, 1]) for each model.
Required if you want to compute log loss for model comparison. The names must match pred_list.}
}
\value{
A list with the following components:
\describe{
  \item{summary_table}{A data.frame summarizing evaluation metrics (Accuracy, Precision, Recall, Specificity, Balanced Accuracy, F1, MCC, Log Loss) for each model.}
  \item{best_models}{A named list indicating the best model for each individual metric.}
}
}
\description{
Evaluates multiple binary classifiers using standard performance metrics and identifies the best model for each metric.
Optionally supports both hard predictions (y_pred) and predicted probabilities (y_prob) for computing log loss.
}
\examples{
y = c(0, 1, 1, 0, 1, 0, 1, 0)
preds = list(
  DecisionTree = c(0, 1, 1, 0, 1, 0, 0, 0),
  RandomForest = c(0, 1, 1, 0, 1, 0, 1, 0)
)
probs = list(
  DecisionTree = c(0.2, 0.9, 0.85, 0.1, 0.95, 0.3, 0.4, 0.5),
  RandomForest = c(0.1, 0.95, 0.87, 0.2, 0.97, 0.1, 0.88, 0.3)
)
result = compare_models(y, preds, prob_list = probs)
print(result$summary_table)
print(result$best_models)

}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/decision_tree.R
\name{build_tree}
\alias{build_tree}
\title{Build a Decision Tree Classifier}
\usage{
build_tree(
  X,
  y,
  depth = 0,
  max_depth = 5,
  min_samples_split = 2,
  criterion = "gini"
)
}
\arguments{
\item{X}{A data.frame containing feature columns (must be numeric).}

\item{y}{A binary target vector (0/1 or TRUE/FALSE).}

\item{depth}{Current depth of the tree (internal use, default = 0).}

\item{max_depth}{Maximum depth allowed for the tree (default = 5).}

\item{min_samples_split}{Minimum number of samples required to split a node (default = 2).}

\item{criterion}{The impurity criterion to use: "gini" or "entropy" (default = "gini").}
}
\value{
A nested list representing the decision tree structure. Each node contains:
\describe{
  \item{type}{Either "node" for internal nodes or "leaf" for terminal nodes.}
  \item{feature}{(for node) The feature used for splitting.}
  \item{value}{(for node) The threshold value used for splitting.}
  \item{left}{(for node) The left subtree.}
  \item{right}{(for node) The right subtree.}
  \item{class}{(for leaf) Predicted class (0 or 1).}
  \item{prob}{(for leaf) Predicted probability of class 1.}
}
}
\description{
This function recursively builds a binary decision tree for binary classification problems.
It uses either Gini impurity or Entropy as the splitting criterion and supports maximum depth and minimum sample split control.
}
\examples{
# Create toy data
X = data.frame(
  x1 = c(2, 5, 4, 3, 7, 2),
  x2 = c(1, 2, 1, 1, 3, 2)
)
y = c(0, 0, 1, 1, 1, 0)

# Train a decision tree
build_tree(X, y, max_depth = 2, min_samples_split = 2, criterion = "gini")

}

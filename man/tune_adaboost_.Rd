% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boosting.R
\name{tune_adaboost_}
\alias{tune_adaboost_}
\title{Tune AdaBoost Hyperparameters via Grid Search on Number of Iterations}
\usage{
tune_adaboost_(
  train_data,
  val_data,
  iter_options = c(10, 20, 30, 50),
  verbose = FALSE
)
}
\arguments{
\item{train_data}{A data frame containing the training set, where the last column is the binary target variable (0 or 1),
and the remaining columns are numeric features.}

\item{val_data}{A data frame containing the validation set, structured the same way as \code{train_data}, with the last column
as the binary target variable (0 or 1), and the rest as numeric features.}

\item{iter_options}{A numeric vector of candidate values for \code{n_iter}, representing the number of boosting rounds
(i.e., the number of weak learners to include in the final model). Must be a vector of positive integers. Default is \code{c(10, 20, 30, 50)}.}

\item{verbose}{Logical. If \code{TRUE}, the function will print the accuracy corresponding to each \code{n_iter} tried. Useful for monitoring progress.}
}
\value{
A list with the following components:
\describe{
  \item{\code{model}}{The trained AdaBoost model (in the same format as returned by \code{\link{adaboost_}}) that achieved the best accuracy.}
  \item{\code{n_iter}}{The value of \code{n_iter} (number of boosting rounds) that gave the best validation performance.}
}
}
\description{
This function automatically tunes the number of boosting iterations (\code{n_iter}) for an AdaBoost classifier
by evaluating different values on a provided validation set. It returns the model with the highest validation accuracy.
}
\details{
The function performs a grid search over the values in \code{iter_options}. For each candidate number of iterations:
\enumerate{
  \item An AdaBoost model is trained using \code{\link{adaboost_}} on the \code{train_data}.
  \item Predictions are made on the \code{val_data} using \code{\link{predict_adaboost_}}.
  \item The prediction accuracy on the validation set is computed.
  \item The model with the highest validation accuracy is retained and returned at the end.
}

If multiple values achieve the same highest accuracy, the one with the smallest \code{n_iter} will be returned (since we iterate in order).
If all models fail to train (e.g., due to empty weak learner lists), the function throws an error.
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
train_data <- data.frame(
  x1 = rnorm(100),
  x2 = rnorm(100),
  y = ifelse(runif(100) > 0.5, 1, 0)
)
val_data <- data.frame(
  x1 = rnorm(50),
  x2 = rnorm(50),
  y = ifelse(runif(50) > 0.5, 1, 0)
)

# Tune n_iter values
result <- tune_adaboost_(
  train_data,
  val_data,
  iter_options = c(5, 10, 15),
  verbose = TRUE
)

print(result$n_iter)
}

}
\seealso{
\code{\link{adaboost_}}, \code{\link{predict_adaboost_}}
}

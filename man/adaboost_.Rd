% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boosting.R
\name{adaboost_}
\alias{adaboost_}
\title{Train an AdaBoost Classifier for Binary Classification}
\usage{
adaboost_(data, n_iter = 50)
}
\arguments{
\item{data}{A data frame containing the training data. All columns except the last are treated as features
(must be numeric), and the last column is treated as the binary label (must be 0 or 1).}

\item{n_iter}{An integer specifying the number of boosting iterations (i.e., number of weak learners to train).
Must be a positive count. Default is 50.}
}
\value{
A list with the following components:
\describe{
  \item{\code{models}}{A list of trained decision stumps. Each stump is a list containing:
    \code{feature} (column index),
    \code{threshold} (split point),
    \code{polarity} (direction of comparison, either 1 or -1).}
  \item{\code{alphas}}{A numeric vector of weights (alpha values) assigned to each weak learner based on its accuracy.}
}
}
\description{
This function implements the AdaBoost (Adaptive Boosting) algorithm using decision stumps (one-level decision trees)
as weak learners. It is designed for binary classification tasks with numeric features and binary labels (0 or 1).
The function trains multiple weak learners sequentially, where each subsequent learner focuses more on instances that
were misclassified by previous learners.
}
\details{
The AdaBoost algorithm assigns a weight to each training instance. Initially, all weights are equal.
At each iteration, a decision stump is trained to minimize the weighted classification error.
The model assigns a weight (alpha) to each stump based on its accuracy.
Then, the weights of the training samples are updated: misclassified samples receive higher weights to focus
the next weak learner on harder cases.

The weak learners used here are decision stumps, which find the optimal threshold for a single feature to
split the data. The algorithm searches over all features and thresholds to find the one with the lowest
weighted classification error.

The function internally converts labels from \code{0, 1} to \code{-1, 1} as required by the AdaBoost algorithm.
}
\examples{
\dontrun{
# Simulate simple binary data
set.seed(123)
X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
y <- ifelse(X$x1 + X$x2 + rnorm(100) > 0, 1, 0)
data <- cbind(X, y)

# Train AdaBoost model
model <- adaboost_(data, n_iter = 20)
print(model)
}

}
\seealso{
\code{\link{predict_adaboost_}}, \code{\link{tune_adaboost_}}
}
